---
title: "Penalised methods for genetic data"
output:
  xaringan::moon_reader:
    df_print: paged
    includes:
      after_body: include_twitter.html
    seal: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', dev = "svg", out.width = "90%",
                      echo = FALSE, comment = "")
options(width = 70)
```

class: title-slide center middle inverse

<br>

# Penalised methods for genetic data

<br>

## Florian Priv√©

### King's College London -- May 20, 2019

<br>

<br>

**Slides:** `https://privefl.github.io/R-presentation/penalised-genetics.html`

---

class: inverse, center, middle

# Introduction to penalized models

---

## Multiple linear regression

We want to solve

$$y = \alpha + \beta_1 G_1 + \cdots + \beta_p G_p + \gamma_1 COV_1 + \cdots + \gamma_q COV_q + \epsilon~.$$
--

Let $\beta = (\alpha, \beta_1, \dots, \beta_p, \gamma_1, \dots, \gamma_q)$ and $X = [1; G_1; \dots;G_p; COV_1; \dots; COV_q]$, then

$$y = X \beta + \epsilon~.$$

--

This is equivalent to minimizing

$$||y - X \beta||_2^2 =  ||\epsilon||_2^2~,$$
--

whose solution is 

$$\beta = (X^T X)^{-1} X^T y~.$$

--

<br>

**What is the problem when analyzing genotype data?**

--

$$n < p$$

---

## Penalization term -- $L_2$ regularization

<br>

Instead, we can minimize

$$||y - X \beta||_2^2 + \lambda ||\beta||_2^2~,$$
--

whose solution is 

$$\beta = (X^T X + \lambda I)^{-1} X^T y~.$$

--

<br>

This is the L2-regularization ("**ridge**", Hoerl and Kennard, 1970); **it shrinks coefficients $\beta$ towards 0**.

.footnote[https://doi.org/10.1080/00401706.1970.10488634]

---

## Penalization term -- $L_1$ regularization

<br>

Instead, we can minimize

$$||y - X \beta||_2^2 + \lambda ||\beta||_1~,$$
--

which does not have any closed form but can be solved using iterative algorithms.

--

<br>

This is the L1-regularization ("**lasso**", Tibshirani, 1996); **it forces some of the coefficients to be equal to 0** and can be used as a means of variable selection, leading to sparse models.

.footnote[https://doi.org/10.1111/j.2517-6161.1996.tb02080.x]

---

## Penalization term -- $L_1$ and $L_2$ regularization

<br>

Instead, we can minimize

$$||y - X \beta||_2^2 + \lambda (\alpha ||\beta||_1 + (1 - \alpha) ||\beta||_2^2)~,$$
--

which does not have any closed form but can be solved using iterative algorithms ( $0 \le \alpha \le 1$ ).

--

<br>

This is the L1- and L2-regularization ("**elastic-net**", Zou and Hastie, 2005); it is a compromise between the two previous penalties.

.footnote[https://doi.org/10.1111/j.1467-9868.2005.00503.x]
